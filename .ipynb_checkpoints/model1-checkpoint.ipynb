{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7b350e-d6b4-41dd-9ae7-2fb869b36bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a6f597-9024-4dc0-9d3d-83618f93e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        \n",
    "        # Get all XML files\n",
    "        ann_dir = os.path.join(data_dir, 'Annotations')\n",
    "        img_dir = os.path.join(data_dir, 'JPEGImages')\n",
    "        \n",
    "        for xml_file in os.listdir(ann_dir):\n",
    "            if xml_file.endswith('.xml'):\n",
    "                ann_path = os.path.join(ann_dir, xml_file)\n",
    "                img_path = os.path.join(img_dir, xml_file.replace('.xml', '.jpg'))\n",
    "                \n",
    "                if os.path.exists(img_path):\n",
    "                    self.annotations.append(ann_path)\n",
    "                    self.images.append(img_path)\n",
    "    \n",
    "    def parse_voc_xml(self, xml_path):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Get image size for normalization\n",
    "        size = root.find('size')\n",
    "        width = float(size.find('width').text)\n",
    "        height = float(size.find('height').text)\n",
    "        \n",
    "        # Get first enemy object (assuming one enemy per image)\n",
    "        obj = root.find('object')\n",
    "        if obj is None:\n",
    "            return None\n",
    "        \n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = float(bbox.find('xmin').text) / width\n",
    "        ymin = float(bbox.find('ymin').text) / height\n",
    "        xmax = float(bbox.find('xmax').text) / width\n",
    "        ymax = float(bbox.find('ymax').text) / height\n",
    "        \n",
    "        # Convert to center format (x_center, y_center, width, height)\n",
    "        x_center = (xmin + xmax) / 2\n",
    "        y_center = (ymin + ymax) / 2\n",
    "        box_width = xmax - xmin\n",
    "        box_height = ymax - ymin\n",
    "        \n",
    "        return [x_center, y_center, box_width, box_height]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        ann_path = self.annotations[idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        bbox = self.parse_voc_xml(ann_path)\n",
    "        \n",
    "        if bbox is None:\n",
    "            bbox = [0, 0, 0, 0]  # Default values if no object found\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(bbox, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ada65d-d297-4be6-a6c4-42d3e5f418a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(data_dir, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create train/val splits\n",
    "    dataset = VOCDataset(data_dir, transform=transform)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22877e26-8ee4-4b86-a5e4-0302829bb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnemyDetector(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(EnemyDetector, self).__init__()\n",
    "        self.backbone = resnet50(pretrained=True)\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "            nn.Sigmoid()  # Bound outputs between 0 and 1 for normalized coordinates\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d30ff1d-6329-4260-9e52-1cbf76d4056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir, num_epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader = create_data_loaders(data_dir, batch_size)\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    model = EnemyDetector().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=2, factor=0.1\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, boxes in train_loader:\n",
    "            images = images.to(device)\n",
    "            boxes = boxes.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, boxes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, boxes in val_loader:\n",
    "                images = images.to(device)\n",
    "                boxes = boxes.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, boxes)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59329c32-bed6-4348-80a2-8a6bc76103da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_detection(model_path):\n",
    "    import mss\n",
    "    sct = mss.mss()\n",
    "    monitor = sct.monitors[1]  # Primary monitor\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = EnemyDetector().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    while True:\n",
    "        # Capture screen\n",
    "        screenshot = np.array(sct.grab(monitor))\n",
    "        image = Image.fromarray(screenshot)\n",
    "        \n",
    "        # Prepare image\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get detection\n",
    "        with torch.no_grad():\n",
    "            bbox = model(input_tensor).cpu().numpy()[0]\n",
    "        \n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        h, w = screenshot.shape[:2]\n",
    "        x_center, y_center, box_w, box_h = bbox\n",
    "        \n",
    "        x1 = int((x_center - box_w/2) * w)\n",
    "        y1 = int((y_center - box_h/2) * h)\n",
    "        x2 = int((x_center + box_w/2) * w)\n",
    "        y2 = int((y_center + box_h/2) * h)\n",
    "        \n",
    "        # Draw bbox\n",
    "        cv2.rectangle(screenshot, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display result\n",
    "        cv2.imshow('Detection', screenshot)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5eafe-9478-4246-a946-9a754d57469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data'\n",
    "model = train_model(\n",
    "    data_dir,\n",
    "    num_epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
